---
title: "Load the data"
author: "Nick van Doormaal"
date: "2023-02-10"
categories: [load, read, data.table, fread, csv, sf, spatial]
description: "In this post, I will show how to get your data in R; the first step of building the interactive report"
execute: 
  message: false
  warning: false
editor_options:
  chunk_output_type: console
---

# Get your data in R

Before we can do anything, we first need to somehow get our data in R. In this blog post, I'll go into detail on several ways to do that. I

## Reading a single csv-file

Perhaps the most common type of files used in data analyses are csv-files. You can read in your data using one of the ways below.

### read_csv

The function `read_csv` comes from the `readr`-package (which is part of the Tidyverse). The `read_csv` function takes several arguments for more advanced manipulations, but let's keep things simple for now.

```{r load-data-csv}
library(readr) ## this package is needed to get access to the read_csv function

patrol_data <- read_csv(
  file = "./posts/230210_load_data/data/230210_example_route.csv"
)
```

`read_csv` assumes that your data is separated by comma's ','. You could also use the function `read_csv2` when your data is separated by semi columns ';'. If you have a different kind of delimiter or want to be explicit about it, you can also use the function `read_delim`, also part of the `readr` package. When you use this function, you have to specify the delimiter with the `delim`-argument.

```{r load-data-delim}
patrol_data_delim <- read_delim(
  file = "./posts/230210_load_data/data/230210_example_route.csv",
  delim = ","
)
all.equal(patrol_data, patrol_data_delim)
```

Both datasets are load exactly in the same way. Another advantage of using the `readr` package is that date-time columns are recognised and formatted to a special class for working with dates and times. This works only if all the entries are formatted in the same way. I tend to format date as year-month-day and time as hour:minute:seconds. This has been working for me in almost every scenario.

### fread

One downside of read_csv is that it might not be the fastest option. I haven't really come across scenario where this really mattered, but if speed and performance is of concern to you then check out `fread` from the `data.table` package.

```{r load-data-fread}
library(data.table)

patrol_data_fread <- fread(
  input = "./posts/230210_load_data/data/230210_example_route.csv"
)
```

The cool thing of `fread` is that it automatically figures out what the delimiter is, so you don't need to specify that. It is also very fast and efficient and the preferred way for working with very large csv-files. The downside, however, is that you cannot read other types of Excel-files like xlsx-files; it has to be a csv. Also, it won't automatically recognise columns with date and time.

## Reading in multiple files

It's not crazy to imagine that you have to load in several files, for example, one for each patrol team. That was the case for the reserve in South Africa as well. Every day I had to download several csv-files, one for every patrol team.

You don't have to specify all the files in R, but you can read in multiple files. First we need to list all the files that we want to read in, and then apply the `read_csv` function to each one. (or another function for reading data). There are two ways we can easily list all the files: through a common word in every file, or put them all in one separate folder.

```{r load-csv-list-files}
## List all the csv-files in a folder
csv_files <- list.files(
  path = "./posts/230210_load_data/data/", ## Here's where you specify the directory to your data files
  pattern = ".csv", ## Here's where you specify the file extension or the common word across the files

)
csv_files

## List all the csv-files that have the word 'route' in common
route_files <- list.files(
  path = "./posts/230210_load_data/data/",
  pattern = "route", ## Here's where you specify the file extension or the common word across the files
  full.names = TRUE ## This needs to be set to TRUE for reading multiple files later
)
```

In the above two example, you can see that you can either focus on the file extension (for example when all the files and no other files are in single folder) or specify a word that's common across the files (for example 'route'). In the last example, I've also set the argument `full.names = TRUE`. This is necessary for when we want to apply the `read_csv` function. To do so, we can use the `map` function from the `purrr` package. With `map` we can basically apply the same function to every file we want.

```{r load-csv-map}
library(purrr)

patrol_data_list <- map(
  route_files, ## here we specify our datafiles
  read_csv ## then we specify the function that we want to apply to every file. You can also use 'read_delim' or 'fread' here as well
)
patrol_data_list
```

The output of `patrol_data_list` is a list of data frames; one data.frame for every file. You can try replacing `route_files` with `csv_files` to see what happens when you don't specify `full.names = TRUE`.

Reading the files in separately is handy and efficient, but is usually not easy to work with. Combining all the dataframes in a list is possible if each has exactly **the same number** of columns, exactly the **same column names** in the **same position**! To do so, we can use the function `bind_rows` from the `dplyr` package.

```{r load-data-bind}
library(dplyr)

all_patrol_data <- bind_rows(patrol_data_list)

all_patrol_data

```

Now all our patrol files are in a single data.frame and ready for data cleaning and analyses!

## Reading spatial data

Next to using 'regular' data files like csv or Excel files, I also regularly use spatial data. THE package for anything related spatial is the `sf` package. The function to read in spatial data like point or polygon is `st_read`. Below is an example where I read in the boundaries of the reserve. The name of the file is 'OlifantsWest.shp'

```{r load-spatial}
library(sf)

reserve_sf <- st_read(
  dsn = "./posts/230210_load_data/data/OlifantsWest.shp"
)

reserve_sf
```

The `st_read` automatically reads in any projection in the same directory or folder if there is any. If you know the coordinate system of the shapefile, but don't have the '.prj' file you set the projection yourself using the `st_set_crs` function by providing an EPSG code. One of the most common EPSG-code is 4326 which is for WGS 84 (longitude and latitude). You can find the EPSG-code for your area on <https://epsg.io/>. Next to 4326, I will also use 32736 to transform the longitude-latitude data to UTM which uses metres as the measuring units.

```{r load-data-crs}
reserve_sf <- st_set_crs(
  x = reserve_sf, value = 4326
)
```
