[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hello there and welcome to my blog! My name is Nick van Doormaal and I have a background in Criminology and Wildlife Conservation. I spend some time in South Africa for my PhD research and studied different poaching problems, including rhino poaching and bushmeat poaching. Nowadays I focus on things closer to home: crimes and activities that are harmful to the Dutch environment. Things like illegal removal of asbestos, dumps of synthetic drug waste, and trafficking of electronic waste.\nI spent most of my time analysing data and I have become a big fan of using R. I have been using it for all sorts of cool stuff like data wrangling, creating interactive visualisations, spatial analyses and making maps.\nI hope you’ll find these posts useful and feel free to reach out if you’re stuck with something or want to learn more about anything related to conservation, criminology, or R.\nCheers!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interactive Reports for Wildlife Security in RStudio",
    "section": "",
    "text": "Load the data\n\n\n\n\n\n\n\nload\n\n\nread\n\n\ndata.table\n\n\nfread\n\n\ncsv\n\n\nsf\n\n\nspatial\n\n\n\n\nIn this post, I will show how to get your data in R; the first step of building the interactive report\n\n\n\n\n\n\nFeb 10, 2023\n\n\nNick van Doormaal\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Interactive Reporting Tool\n\n\n\n\n\n\n\nintroduction\n\n\ninteractive\n\n\nreporting\n\n\nlearning\n\n\nwildlife\n\n\nSouth Africa\n\n\npoaching\n\n\npatrol\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nNick van Doormaal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/230210_load_data/230210_post.html",
    "href": "posts/230210_load_data/230210_post.html",
    "title": "Load the data",
    "section": "",
    "text": "Before we can do anything, we first need to somehow get our data in R. In this blog post, I’ll go into detail on several ways to do that. I\n\n\nPerhaps the most common type of files used in data analyses are csv-files. You can read in your data using one of the ways below.\n\n\nThe function read_csv comes from the readr-package (which is part of the Tidyverse). The read_csv function takes several arguments for more advanced manipulations, but let’s keep things simple for now.\n\nlibrary(readr) ## this package is needed to get access to the read_csv function\n\npatrol_data <- read_csv(\n  file = \"./data/230210_example_route.csv\"\n)\n\nread_csv assumes that your data is separated by comma’s ‘,’. You could also use the function read_csv2 when your data is separated by semi columns ‘;’. If you have a different kind of delimiter or want to be explicit about it, you can also use the function read_delim, also part of the readr package. When you use this function, you have to specify the delimiter with the delim-argument.\n\npatrol_data_delim <- read_delim(\n  file = \"./data/230210_example_route.csv\",\n  delim = \",\"\n)\nall.equal(patrol_data, patrol_data_delim)\n\n[1] TRUE\n\n\nBoth datasets are load exactly in the same way. Another advantage of using the readr package is that date-time columns are recognised and formatted to a special class for working with dates and times. This works only if all the entries are formatted in the same way. I tend to format date as year-month-day and time as hour:minute:seconds. This has been working for me in almost every scenario.\n\n\n\nOne downside of read_csv is that it might not be the fastest option. I haven’t really come across scenario where this really mattered, but if speed and performance is of concern to you then check out fread from the data.table package.\n\nlibrary(data.table)\n\npatrol_data_fread <- fread(\n  input = \"./data/230210_example_route.csv\"\n)\n\nThe cool thing of fread is that it automatically figures out what the delimiter is, so you don’t need to specify that. It is also very fast and efficient and the preferred way for working with very large csv-files. The downside, however, is that you cannot read other types of Excel-files like xlsx-files; it has to be a csv. Also, it won’t automatically recognise columns with date and time.\n\n\n\n\nIt’s not crazy to imagine that you have to load in several files, for example, one for each patrol team. That was the case for the reserve in South Africa as well. Every day I had to download several csv-files, one for every patrol team.\nYou don’t have to specify all the files in R, but you can read in multiple files. First we need to list all the files that we want to read in, and then apply the read_csv function to each one. (or another function for reading data). There are two ways we can easily list all the files: through a common word in every file, or put them all in one separate folder.\n\n## List all the csv-files in a folder\ncsv_files <- list.files(\n  path = \"./data/\", ## Here's where you specify the directory to your data files\n  pattern = \".csv\", ## Here's where you specify the file extension or the common word across the files\n\n)\ncsv_files\n\n[1] \"230210_another_example_route.csv\" \"230210_example_route.csv\"        \n\n## List all the csv-files that have the word 'route' in common\nroute_files <- list.files(\n  path = \"./data/\",\n  pattern = \"route\", ## Here's where you specify the file extension or the common word across the files\n  full.names = TRUE ## This needs to be set to TRUE for reading multiple files later\n)\n\nIn the above two example, you can see that you can either focus on the file extension (for example when all the files and no other files are in single folder) or specify a word that’s common across the files (for example ‘route’). In the last example, I’ve also set the argument full.names = TRUE. This is necessary for when we want to apply the read_csv function. To do so, we can use the map function from the purrr package. With map we can basically apply the same function to every file we want.\n\nlibrary(purrr)\n\npatrol_data_list <- map(\n  route_files, ## here we specify our datafiles\n  read_csv ## then we specify the function that we want to apply to every file. You can also use 'read_delim' or 'fread' here as well\n)\npatrol_data_list\n\n[[1]]\n# A tibble: 1,036 × 6\n   device_id timestamp           accuracy heading_d latitude longitude\n   <chr>     <dttm>                 <dbl>     <dbl>    <dbl>     <dbl>\n 1 Tango     1999-12-31 20:12:04        4     356      -24.2      30.9\n 2 Tango     1999-12-31 20:12:08        4     360      -24.2      30.9\n 3 Tango     1999-12-31 20:12:11        5     354      -24.2      30.9\n 4 Tango     1999-12-31 20:12:14        6     352.     -24.2      30.9\n 5 Tango     1999-12-31 20:12:18        4       3.9    -24.2      30.9\n 6 Tango     1999-12-31 20:12:21        4     358      -24.2      30.9\n 7 Tango     1999-12-31 20:12:25        4     356      -24.2      30.9\n 8 Tango     1999-12-31 20:12:29        4     356.     -24.2      30.9\n 9 Tango     1999-12-31 20:12:32        4     359      -24.2      30.9\n10 Tango     1999-12-31 20:12:36        4     357      -24.2      30.9\n# … with 1,026 more rows\n\n[[2]]\n# A tibble: 998 × 6\n   device_id timestamp           accuracy heading_d latitude longitude\n   <chr>     <dttm>                 <dbl>     <dbl>    <dbl>     <dbl>\n 1 Lema      2000-01-01 08:20:23        6        87    -24.2      30.9\n 2 Lema      2000-01-01 08:20:27        6        85    -24.2      30.9\n 3 Lema      2000-01-01 08:20:30        6        81    -24.2      30.9\n 4 Lema      2000-01-01 08:20:33        6        82    -24.2      30.9\n 5 Lema      2000-01-01 08:20:37        6        79    -24.2      30.9\n 6 Lema      2000-01-01 08:20:41        6        86    -24.2      30.9\n 7 Lema      2000-01-01 08:20:45        7        83    -24.2      30.9\n 8 Lema      2000-01-01 08:20:48        6        85    -24.2      30.9\n 9 Lema      2000-01-01 08:20:51        6        85    -24.2      30.9\n10 Lema      2000-01-01 08:20:55        6        86    -24.2      30.9\n# … with 988 more rows\n\n\nThe output of patrol_data_list is a list of data frames; one data.frame for every file. You can try replacing route_files with csv_files to see what happens when you don’t specify full.names = TRUE.\nReading the files in separately is handy and efficient, but is usually not easy to work with. Combining all the dataframes in a list is possible if each has exactly the same number of columns, exactly the same column names in the same position! To do so, we can use the function bind_rows from the dplyr package.\n\nlibrary(dplyr)\n\nall_patrol_data <- bind_rows(patrol_data_list)\n\nall_patrol_data\n\n# A tibble: 2,034 × 6\n   device_id timestamp           accuracy heading_d latitude longitude\n   <chr>     <dttm>                 <dbl>     <dbl>    <dbl>     <dbl>\n 1 Tango     1999-12-31 20:12:04        4     356      -24.2      30.9\n 2 Tango     1999-12-31 20:12:08        4     360      -24.2      30.9\n 3 Tango     1999-12-31 20:12:11        5     354      -24.2      30.9\n 4 Tango     1999-12-31 20:12:14        6     352.     -24.2      30.9\n 5 Tango     1999-12-31 20:12:18        4       3.9    -24.2      30.9\n 6 Tango     1999-12-31 20:12:21        4     358      -24.2      30.9\n 7 Tango     1999-12-31 20:12:25        4     356      -24.2      30.9\n 8 Tango     1999-12-31 20:12:29        4     356.     -24.2      30.9\n 9 Tango     1999-12-31 20:12:32        4     359      -24.2      30.9\n10 Tango     1999-12-31 20:12:36        4     357      -24.2      30.9\n# … with 2,024 more rows\n\n\nNow all our patrol files are in a single data.frame and ready for data cleaning and analyses!\n\n\n\nNext to using ‘regular’ data files like csv or Excel files, I also regularly use spatial data. THE package for anything related spatial is the sf package. The function to read in spatial data like point or polygon is st_read. Below is an example where I read in the boundaries of the reserve. The name of the file is ‘OlifantsWest.shp’\n\nlibrary(sf)\n\nreserve_sf <- st_read(\n  dsn = \"./data/OlifantsWest.shp\"\n)\n\nReading layer `OlifantsWest' from data source \n  `C:\\Users\\Daikoro\\Documents\\R\\daikoro-blog\\posts\\230210_load_data\\data\\OlifantsWest.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 278735 ymin: 7317223 xmax: 295162 ymax: 7327195\nProjected CRS: WGS 84 / UTM zone 36S\n\nreserve_sf\n\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 278735 ymin: 7317223 xmax: 295162 ymax: 7327195\nProjected CRS: WGS 84 / UTM zone 36S\n         Region  f  d    ID TEXT border    area layer path Priority Name\n1 Olifants West NA NA 98013  183   <NA> 9717547  <NA> <NA>       NA <NA>\n                        geometry\n1 POLYGON ((284528.5 7317223,...\n\n\nThe st_read automatically reads in any projection in the same directory or folder if there is any. If you know the coordinate system of the shapefile, but don’t have the ‘.prj’ file you set the projection yourself using the st_set_crs function by providing an EPSG code. One of the most common EPSG-code is 4326 which is for WGS 84 (longitude and latitude). You can find the EPSG-code for your area on https://epsg.io/. Next to 4326, I will also use 32736 to transform the longitude-latitude data to UTM which uses metres as the measuring units.\n\nreserve_sf <- st_set_crs(\n  x = reserve_sf, value = 4326\n)\n\nThat’s it for reading data in R. If you encountered any problems or have your own way of reading in multiple files, just let me know!\n\n\n\nAfrican goshawk (taken in South Africa)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Introduction to Interactive Reporting Tool",
    "section": "",
    "text": "Hello there and welcome to the first blog post of the series. In this series, I’ll go into detail on how the interactive report tool works and go step-by-step on how to build your own reporting tool. Of course, you can always adapt it to your own situation and needs."
  },
  {
    "objectID": "posts/welcome/index.html#reporting-tool",
    "href": "posts/welcome/index.html#reporting-tool",
    "title": "Introduction to Interactive Reporting Tool",
    "section": "Reporting tool",
    "text": "Reporting tool\nThe reporting tool provides patrol managers with a quick overview of patrol activity in the last 24 hours. Think of things like patrol routes and observations of poaching activity, but also includes ecological data like reported signs of endangered wildlife species. This helps managers and planners with providing answers to some basic questions like: Which teams went where for their patrols? Did they cover the most important areas? Who reported any suspicious activities and where? The first page of the report contains clear visuals like maps, graphs showing patrol activity and the number of wildlife observations. The other page has a couple of tables with detailed information on every patrol like kilometres walked, time on patrol etc. But also more details on the patrol observations if they encountered any suspicious activities."
  },
  {
    "objectID": "posts/welcome/index.html#why-this-tool",
    "href": "posts/welcome/index.html#why-this-tool",
    "title": "Introduction to Interactive Reporting Tool",
    "section": "Why this tool?",
    "text": "Why this tool?\nI first developed the initial version of the reporting tool during my first field trip to South Africa in 2016. During my stay I met the team I worked with for my PhD research and they were writing a summary report every single day to share with all the patrol managers. This took them a lot of work and even impacted my own work, because we always had to get back from the field a few hours earlier for the team to write these reports. My supervisor and I believed this could be done much easier by automating it in R. So in my spare time, I have been chatting with the team members to figure what they would like to see in the report and how information it currently contains. This took me a couple of weeks to get a first version up and running, and it definitely wasn’t perfect. But, more importantly, it was working, showed more or less the same kind of information as before, and now only took a few minutes in R!\nI later updated the R-script a couple of times whenever they encountered errors or to include some more detailed information. However, it was still report with static information and graphs. This caused a couple of problems. For example, the maps and figures sometimes looked a bit crowed, especially when more teams went out than usual. If we could include some kind of interactivity, we could then select only a handful teams at the team and better understand which areas each team covered and what they reported. That is what I have been working on recently, to update the current version of the report to a more interactive report."
  },
  {
    "objectID": "posts/welcome/index.html#about-this-series",
    "href": "posts/welcome/index.html#about-this-series",
    "title": "Introduction to Interactive Reporting Tool",
    "section": "About this series",
    "text": "About this series\nIn this series, I’ll go into detail on how the interactive report tool works and go step-by-step on how to build your own reporting tool. Each post will stand on it’s own, like how to read in your data, how to make a map, and some minor spatial analyses. All the steps combined should help you to make your own interactive reporting tool.\nI’ll refer a lot to patrolling data and poaching in South Africa, because that was the context in which the original tool was built. The data that I used in this series are all simulated and do not reflect the actual situation on the ground. I did this on purpose to avoid spreading sensitive information on security and the distribution of endangered species. The data is just to illustrate how the reporting tool works so that you can perhaps build your own."
  },
  {
    "objectID": "posts/welcome/index.html#new-to-r-and-rstudio",
    "href": "posts/welcome/index.html#new-to-r-and-rstudio",
    "title": "Introduction to Interactive Reporting Tool",
    "section": "New to R and RStudio?",
    "text": "New to R and RStudio?\nI assume you are already somewhat familiar with R and RStudio. If you are a complete beginner and are interested in learning R and how to install it, I suggest the following resources:\n\nInstalling R and RStudio: A guide on how to download and install R and RStudio on your computer. It will also guide you around the RStudio interface.\nIntroduction to R: Not the nicest looking guide, but very detailed guide on how the R-language works\nTidyverse: The tidyverse is a collection of open source packages for the R programming language introduced by Hadley Wickham and his team that “share an underlying design philosophy, grammar, and data structures” of tidy data. It encourages making your R-script much more readable and understandable. Throughout this blog series, I’ll rely heavily on the packages within the Tidyverse.\nR for Data Science: This is a great resource focussing on how to use R for Data Science. It’s very detailed and clearly explains how to do all sort of things in R; from simple things like reading your data to more advanced topics like web scraping.\n\nI hope you’ll find these posts useful and feel free to reach out if you’re stuck with something or want to learn more about anything related to conservation, criminology, or R.\n\n\n\nThe wonderful wildlife of South Africa"
  }
]