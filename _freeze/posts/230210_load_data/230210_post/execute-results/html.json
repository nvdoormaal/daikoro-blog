{
  "hash": "be7cfd7af8266b481a0dece005ea603f",
  "result": {
    "markdown": "---\ntitle: \"Load the data\"\nauthor: \"Nick van Doormaal\"\ndate: \"2023-02-10\"\ncategories: [load, read, data.table, fread, csv, sf, spatial]\ndescription: \"In this post, I will show how to get your data in R; the first step of building the interactive report\"\nexecute: \n  message: false\n  warning: false\neditor_options:\n  chunk_output_type: console\n---\n\n\n# Get your data in R\n\nBefore we can do anything, we first need to somehow get our data in R. In this blog post, I'll go into detail on several ways to do that. I\n\n## Reading a single csv-file\n\nPerhaps the most common type of files used in data analyses are csv-files. You can read in your data using one of the ways below.\n\n### read_csv\n\nThe function `read_csv` comes from the `readr`-package (which is part of the Tidyverse). The `read_csv` function takes several arguments for more advanced manipulations, but let's keep things simple for now.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) ## this package is needed to get access to the read_csv function\n\npatrol_data <- read_csv(\n  file = \"./data/230210_example_route.csv\"\n)\n```\n:::\n\n\n`read_csv` assumes that your data is separated by comma's ','. You could also use the function `read_csv2` when your data is separated by semi columns ';'. If you have a different kind of delimiter or want to be explicit about it, you can also use the function `read_delim`, also part of the `readr` package. When you use this function, you have to specify the delimiter with the `delim`-argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npatrol_data_delim <- read_delim(\n  file = \"./data/230210_example_route.csv\",\n  delim = \",\"\n)\nall.equal(patrol_data, patrol_data_delim)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nBoth datasets are load exactly in the same way. Another advantage of using the `readr` package is that date-time columns are recognised and formatted to a special class for working with dates and times. This works only if all the entries are formatted in the same way. I tend to format date as year-month-day and time as hour:minute:seconds. This has been working for me in almost every scenario.\n\n### fread\n\nOne downside of read_csv is that it might not be the fastest option. I haven't really come across scenario where this really mattered, but if speed and performance is of concern to you then check out `fread` from the `data.table` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\n\npatrol_data_fread <- fread(\n  input = \"./data/230210_example_route.csv\"\n)\n```\n:::\n\n\nThe cool thing of `fread` is that it automatically figures out what the delimiter is, so you don't need to specify that. It is also very fast and efficient and the preferred way for working with very large csv-files. The downside, however, is that you cannot read other types of Excel-files like xlsx-files; it has to be a csv. Also, it won't automatically recognise columns with date and time.\n\n## Reading in multiple files\n\nIt's not crazy to imagine that you have to load in several files, for example, one for each patrol team. That was the case for the reserve in South Africa as well. Every day I had to download several csv-files, one for every patrol team.\n\nYou don't have to specify all the files in R, but you can read in multiple files. First we need to list all the files that we want to read in, and then apply the `read_csv` function to each one. (or another function for reading data). There are two ways we can easily list all the files: through a common word in every file, or put them all in one separate folder.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## List all the csv-files in a folder\ncsv_files <- list.files(\n  path = \"./data/\", ## Here's where you specify the directory to your data files\n  pattern = \".csv\", ## Here's where you specify the file extension or the common word across the files\n\n)\ncsv_files\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"230210_another_example_route.csv\" \"230210_example_route.csv\"        \n```\n:::\n\n```{.r .cell-code}\n## List all the csv-files that have the word 'route' in common\nroute_files <- list.files(\n  path = \"./data/\",\n  pattern = \"route\", ## Here's where you specify the file extension or the common word across the files\n  full.names = TRUE ## This needs to be set to TRUE for reading multiple files later\n)\n```\n:::\n\n\nIn the above two example, you can see that you can either focus on the file extension (for example when all the files and no other files are in single folder) or specify a word that's common across the files (for example 'route'). In the last example, I've also set the argument `full.names = TRUE`. This is necessary for when we want to apply the `read_csv` function. To do so, we can use the `map` function from the `purrr` package. With `map` we can basically apply the same function to every file we want.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\n\npatrol_data_list <- map(\n  route_files, ## here we specify our datafiles\n  read_csv ## then we specify the function that we want to apply to every file. You can also use 'read_delim' or 'fread' here as well\n)\npatrol_data_list\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n# A tibble: 1,036 × 6\n   device_id timestamp           accuracy heading_d latitude longitude\n   <chr>     <dttm>                 <dbl>     <dbl>    <dbl>     <dbl>\n 1 Tango     1999-12-31 20:12:04        4     356      -24.2      30.9\n 2 Tango     1999-12-31 20:12:08        4     360      -24.2      30.9\n 3 Tango     1999-12-31 20:12:11        5     354      -24.2      30.9\n 4 Tango     1999-12-31 20:12:14        6     352.     -24.2      30.9\n 5 Tango     1999-12-31 20:12:18        4       3.9    -24.2      30.9\n 6 Tango     1999-12-31 20:12:21        4     358      -24.2      30.9\n 7 Tango     1999-12-31 20:12:25        4     356      -24.2      30.9\n 8 Tango     1999-12-31 20:12:29        4     356.     -24.2      30.9\n 9 Tango     1999-12-31 20:12:32        4     359      -24.2      30.9\n10 Tango     1999-12-31 20:12:36        4     357      -24.2      30.9\n# … with 1,026 more rows\n\n[[2]]\n# A tibble: 998 × 6\n   device_id timestamp           accuracy heading_d latitude longitude\n   <chr>     <dttm>                 <dbl>     <dbl>    <dbl>     <dbl>\n 1 Lema      2000-01-01 08:20:23        6        87    -24.2      30.9\n 2 Lema      2000-01-01 08:20:27        6        85    -24.2      30.9\n 3 Lema      2000-01-01 08:20:30        6        81    -24.2      30.9\n 4 Lema      2000-01-01 08:20:33        6        82    -24.2      30.9\n 5 Lema      2000-01-01 08:20:37        6        79    -24.2      30.9\n 6 Lema      2000-01-01 08:20:41        6        86    -24.2      30.9\n 7 Lema      2000-01-01 08:20:45        7        83    -24.2      30.9\n 8 Lema      2000-01-01 08:20:48        6        85    -24.2      30.9\n 9 Lema      2000-01-01 08:20:51        6        85    -24.2      30.9\n10 Lema      2000-01-01 08:20:55        6        86    -24.2      30.9\n# … with 988 more rows\n```\n:::\n:::\n\n\nThe output of `patrol_data_list` is a list of data frames; one data.frame for every file. You can try replacing `route_files` with `csv_files` to see what happens when you don't specify `full.names = TRUE`.\n\nReading the files in separately is handy and efficient, but is usually not easy to work with. Combining all the dataframes in a list is possible if each has exactly **the same number** of columns, exactly the **same column names** in the **same position**! To do so, we can use the function `bind_rows` from the `dplyr` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nall_patrol_data <- bind_rows(patrol_data_list)\n\nall_patrol_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,034 × 6\n   device_id timestamp           accuracy heading_d latitude longitude\n   <chr>     <dttm>                 <dbl>     <dbl>    <dbl>     <dbl>\n 1 Tango     1999-12-31 20:12:04        4     356      -24.2      30.9\n 2 Tango     1999-12-31 20:12:08        4     360      -24.2      30.9\n 3 Tango     1999-12-31 20:12:11        5     354      -24.2      30.9\n 4 Tango     1999-12-31 20:12:14        6     352.     -24.2      30.9\n 5 Tango     1999-12-31 20:12:18        4       3.9    -24.2      30.9\n 6 Tango     1999-12-31 20:12:21        4     358      -24.2      30.9\n 7 Tango     1999-12-31 20:12:25        4     356      -24.2      30.9\n 8 Tango     1999-12-31 20:12:29        4     356.     -24.2      30.9\n 9 Tango     1999-12-31 20:12:32        4     359      -24.2      30.9\n10 Tango     1999-12-31 20:12:36        4     357      -24.2      30.9\n# … with 2,024 more rows\n```\n:::\n:::\n\n\nNow all our patrol files are in a single data.frame and ready for data cleaning and analyses!\n\n## Reading spatial data\n\nNext to using 'regular' data files like csv or Excel files, I also regularly use spatial data. THE package for anything related spatial is the `sf` package. The function to read in spatial data like point or polygon is `st_read`. Below is an example where I read in the boundaries of the reserve. The name of the file is 'OlifantsWest.shp'\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\n\nreserve_sf <- st_read(\n  dsn = \"./data/OlifantsWest.shp\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `OlifantsWest' from data source \n  `C:\\Users\\Daikoro\\Documents\\R\\daikoro-blog\\posts\\230210_load_data\\data\\OlifantsWest.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 278735 ymin: 7317223 xmax: 295162 ymax: 7327195\nProjected CRS: WGS 84 / UTM zone 36S\n```\n:::\n\n```{.r .cell-code}\nreserve_sf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 278735 ymin: 7317223 xmax: 295162 ymax: 7327195\nProjected CRS: WGS 84 / UTM zone 36S\n         Region  f  d    ID TEXT border    area layer path Priority Name\n1 Olifants West NA NA 98013  183   <NA> 9717547  <NA> <NA>       NA <NA>\n                        geometry\n1 POLYGON ((284528.5 7317223,...\n```\n:::\n:::\n\n\nThe `st_read` automatically reads in any projection in the same directory or folder if there is any. If you know the coordinate system of the shapefile, but don't have the '.prj' file you set the projection yourself using the `st_set_crs` function by providing an EPSG code. One of the most common EPSG-code is 4326 which is for WGS 84 (longitude and latitude). You can find the EPSG-code for your area on <https://epsg.io/>. Next to 4326, I will also use 32736 to transform the longitude-latitude data to UTM which uses metres as the measuring units.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreserve_sf <- st_set_crs(\n  x = reserve_sf, value = 4326\n)\n```\n:::\n\n\nThat's it for reading data in R. If you encountered any problems or have your own way of reading in multiple files, just let me know!\n\n![African goshawk (taken in South Africa)](230210_goshawk.JPG){fig-align=\"center\" width=\"384\"}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}